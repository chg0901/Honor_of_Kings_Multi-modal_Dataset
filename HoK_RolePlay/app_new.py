import os
import torch
import random 
import gradio as gr
import time
from zhconv import convert
from LLM import LLM
from src.cost_time import calculate_time
#from openxlab.model import download
#import pdb
os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"
#os.environ['LD_LIBRARY_PATH'] = '/usr/local/lib:/usr/lib:' + os.environ.get('LD_LIBRARY_PATH', '')


def get_title(title = ''):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://github.com/YongXie66/Honor-of-Kings_RolePlay" target="_blank">ä¸»é¡µ</a>]
        </span>
        <br> 
    </p>
    """
    return description


# è®¾ç½®é»˜è®¤system
default_system = 'ä½ æ­£åœ¨æ‰®æ¼”ç‹è€…è£è€€é‡Œçš„è§’è‰²å¦²å·±'
# è®¾ç½®é»˜è®¤çš„prompt
prefix_prompt = '''è¯·ç”¨å°‘äº50ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'''

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'éŸ³é¢‘å¯èƒ½è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»è¯­éŸ³è¯†åˆ«'
        gr.Warning(question)
    return question

@calculate_time
def TTS_response(text, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
                 question_audio, question, 
                 tts_method = '', save_path = 'results/answer.wav'):
    if tts_method == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        try:
            vits.predict(ref_wav_path = inp_ref,
                            prompt_text = prompt_text,
                            prompt_language = prompt_language,
                            text = text, # å›ç­”
                            text_language = text_language,
                            how_to_cut = how_to_cut,
                            save_path = 'results/answer.wav')
            print(text, tts_method, save_path)
            return 'results/answer.wav', None
        except Exception as e:
            gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
            return None, None
    return None, None
@calculate_time
def LLM_response(question_audio, question, 
                 inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", 
                 tts_method = ''):
    answer = llm.generate(question, default_system)
    print(answer)
    driven_audio, driven_vtt = TTS_response(answer, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, question, 
                 tts_method)
    return driven_audio, driven_vtt, answer


@calculate_time
def Talker_response_img(question_audio, method, text, 
                        inp_ref , prompt_text, prompt_language, text_language, how_to_cut,
                        tts_method,
                        source_image,
                        preprocess_type, 
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps, progress=gr.Progress(track_tqdm=True)
                    ):

    driven_audio, driven_vtt, answer = LLM_response(question_audio, text,  
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut,
                                               tts_method = tts_method)
    # pdb.set_trace()
    if method == 'SadTalker':
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    else:
        return None
    if driven_vtt:
        return video, driven_vtt, answer
    else:
        return video, answer


def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history


def clear_session():
    # clear history
    llm.clear_history()
    return '', []

def clear_text():
    return "", ""


GPT_SoVITS_ckpt = "GPT_SoVITS/pretrained_models"
def load_vits_model(gpt_path, sovits_path, progress=gr.Progress(track_tqdm=True)):
    global vits
    print("gpt_sovitsæ¨¡å‹åŠ è½½ä¸­...", gpt_path, sovits_path)
    all_gpt_path, all_sovits_path = os.path.join(GPT_SoVITS_ckpt, gpt_path), os.path.join(GPT_SoVITS_ckpt, sovits_path)
    vits.load_model(all_gpt_path, all_sovits_path)
    gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    return gpt_path, sovits_path


def webui_setting(talk = True):
    if not talk:
        with gr.Tabs():
            with gr.TabItem('æ•°å­—äººå½¢è±¡è®¾å®š'):
                source_image = gr.Image(label="Source image", type="filepath")
    else:
        source_image = gr.Image(value='inputs/DaJi.png', label="DaJi image", type="filepath", elem_id="img2img_image", width=256, interactive=False, visible=False)  


    # inp_ref = gr.Textbox(value='./GPT_SoVITS/ref_audio/ä¸»äººçš„å‘½ä»¤,æ˜¯ç»å¯¹çš„.wav', visible=False)
    inp_ref = gr.Audio(value="GPT_SoVITS/ref_audio/ref_audio.wav", type="filepath", visible=False)
    prompt_text = gr.Textbox(value='ä¸»äººçš„å‘½ä»¤ï¼Œæ˜¯ç»å¯¹çš„', visible=False)
    prompt_language = gr.Textbox(value="ä¸­æ–‡", visible=False)
    text_language = gr.Textbox(value="ä¸­æ–‡", visible=False)
    how_to_cut = gr.Textbox(value="å‡‘å››å¥ä¸€åˆ‡", visible=False)
    batch_size = gr.Textbox(value=2, visible=False)

    tts_method = gr.Textbox(value='GPT-SoVITSå…‹éš†å£°éŸ³', visible=False)
    talker_method = gr.Textbox(value='SadTalker', visible=False)
    llm_method = gr.Textbox(value='InternLM2', visible=False)
    return  (source_image, 
             inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
             tts_method, batch_size, talker_method, llm_method)


def app_chatty():
    with gr.Blocks(analytics_enabled=False, title = 'DaJi_RolePlay') as inference:
        gr.HTML(get_title("Chatty_DaJi~å°ç‹ä»™ğŸŒŸé™ªä½ èŠå¤©"))
        with gr.Row():
            with gr.Column():
                source_image = gr.Image(value='inputs/DaJi.png', type="filepath", elem_id="img2img_image", interactive=False, visible=True, label="å°ç‹ä»™")  

            with gr.Column():
                system_input = gr.Textbox(value=default_system, lines=1, label='System', visible=False)
                chatbot = gr.Chatbot(height=400, show_copy_button=True, label='èŠå¤©æ¡†')
                with gr.Group():
                    question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=False)
                    asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                msg = gr.Textbox(label="Prompt/è¾“å…¥é—®é¢˜")
                asr_text.click(fn=Asr,inputs=[question_audio],outputs=[msg])
                
                with gr.Row():
                    sumbit = gr.Button("ğŸš€ å‘é€", variant = 'primary')
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    
            # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
            sumbit.click(chat_response, inputs=[system_input, msg, chatbot], 
                         outputs=[msg, chatbot])
            
            # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
            clear_history.click(fn = clear_session, outputs = [msg, chatbot])
            
        # exmaple_setting(asr_method, msg, character, talker_method, tts_method, voice, llm_method)
    return inference


def app_lively():
    with gr.Blocks(analytics_enabled=False, title = 'DaJi_RolePlay') as inference:
        gr.HTML(get_title("Lively_DaJi~å°ç‹ä»™ğŸŒŸé™ªä½ èŠå¤©"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                # with gr.Tabs(elem_id="sadtalker_source_image"):
                #         with gr.TabItem('Source image'):
                #             with gr.Row():
                #                 source_image_path = "inputs/DaJi.png" 
                #                 source_image = gr.Image(value=source_image_path, label="DaJi image", type="filepath", elem_id="img2img_image", width=256, interactive=False)                                
                (source_image,  
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
                tts_method, batch_size, talker_method, llm_method)= webui_setting()
                             
                with gr.Tabs():
                    with gr.TabItem('ASR'):
                        # chatbot = gr.Chatbot(height=400, show_copy_button=True)
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³è¾“å…¥')
                            asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³è¾“å…¥åç‚¹å‡»ï¼‰')

                with gr.Tabs(): 
                    with gr.TabItem('Text'):
                        # gr.Markdown("## Text Examples")
                        examples =  [
                            ['ä½ å¥½å‘€ï¼Œä½ æ˜¯è°ï¼Ÿ'],
                            ['æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œæ¥å’Œæˆ‘èŠå¤©å§ï¼'],
                            ['ä½ çŸ¥é“å¦‚ä½•åº”å¯¹å‹åŠ›å—ï¼Ÿ'],
                        ]
                        
                        input_text = gr.Textbox(label="Input Text", lines=5)
                        output_text = gr.Textbox(label="Output Text", lines=8)
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                        gr.Examples(
                            examples = examples,
                            inputs = [input_text],
                        )
                        
                        with gr.Row():
                            submit = gr.Button('ğŸš€ å‘é€', elem_id="LLM&sadtalker_generate", variant='primary')
                            clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å¯¹è¯")
                        
                        clear_history.click(fn=clear_text, outputs=[input_text, output_text])
            
            with gr.Column(variant='panel'):
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('æ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings", open=False):
                            with gr.Row():
                                size_of_image = gr.Radio([256, 512], value=256, label='face model resolution')
                                batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=8) 
                                enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(take a long time)", value=False)        
                                pose_style = gr.Number(value=0, visible=False)
                                exp_weight = gr.Number(value=1, visible=False)
                                blink_every = gr.Checkbox(value=True, visible=False)
                                preprocess_type = gr.Textbox(value='full', visible=False)
                                is_still_mode = gr.Checkbox(value=True, visible=False)
                                facerender = gr.Textbox(value='facevid2vid', visible=False)
                                fps = gr.Number(value=20, visible=False)

                with gr.Tabs(elem_id="sadtalker_genearted"):
                    gen_video = gr.Video(label="Generated video", format="mp4", value='inputs/DaJi_initial.mp4')  # avi,mp4

                submit.click(
                fn=Talker_response_img,
                inputs=[question_audio,
                        talker_method, 
                        input_text, 
                        inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
                        tts_method,
                        source_image, 
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps], 
                outputs=[gen_video,
                         output_text]
                )
        
    return inference


def success_print(text):
    print(f"\033[1;31;42m{text}\033[0m")

def error_print(text):
    print(f"\033[1;37;41m{text}\033[0m")


if __name__ == "__main__":

    # ç¯å¢ƒå‘½ä»¤
    # os.chdir('/home/xlab-app-center/')
    #os.system('ln -s /usr/local/lib /usr/lib')
    #os.system('find libsox.so')
    
    LLM_openxlab_path = "shenfeilang/Honor-of-Kings_RolePlay"
    lively_openxlab_path = "YongXie66/DaJi_RolePlay"
    llm_path = "./InternLM2/InternLM2_7b"
    lively_path = "./DaJi_RolePlay"

    # LLMæ¨¡å‹ä¸‹è½½
    # download(model_repo=LLM_openxlab_path,
    #      output='./InternLM2/InternLM2_7b')
    os.system('apt install git')
    os.system('apt install git-lfs')
    os.system(f'git clone https://code.openxlab.org.cn/shenfeilang/Honor-of-Kings_RolePlay.git {llm_path}')
    os.system(f'cd {llm_path} && git lfs pull')

    # # gpt_sovits, sadtalker æ¨¡å‹ä¸‹è½½
    # download(model_repo=lively_openxlab_path,
        #  output= lively_path)
    os.system(f'git clone https://code.openxlab.org.cn/YongXie66/DaJi_RolePlay.git {lively_path}')
    os.system(f'cd {lively_path} && git lfs pull')

    # è·å–å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨
    directory_list = os.listdir('.')
    print(directory_list)

    # æ¨¡å‹ä½ç½®ç§»åŠ¨
    os.system(f"mv -f {lively_path}/GPT_SoVITS/pretrained_models/* ./GPT_SoVITS/pretrained_models/")
    os.system(f"mv -f {lively_path}/checkpoints/* ./checkpoints")
    # os.system(f"mv {lively_path}//FunASR/* ./FunASR/")
    os.system(f"mv {lively_path}//gfpgan/* ./gfpgan/")

    llm_class = LLM(mode='offline')
    try:
        llm = llm_class.init_model('InternLM2', 'InternLM2/InternLM2_7b', prefix_prompt=prefix_prompt)
        success_print("Success!!! LLMæ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"Error: {e}")
        error_print("å¦‚æœä½¿ç”¨InternLM2_DaJiï¼Œè¯·å…ˆä¸‹è½½InternLM2æ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    try:
        from VITS import *
        vits = GPT_SoVITS()
        gpt_path = "DaJi-e15.ckpt"
        sovits_path = "DaJi_e12_s240.pth"
        load_vits_model(gpt_path, sovits_path)
        success_print("Success!!! GPT-SoVITSæ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"GPT-SoVITS Error: {e}")
        error_print("è¯·å…ˆä¸‹è½½GPT-SoVITSæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    torch.cuda.empty_cache()
    
    try:
        from TFG import SadTalker
        talker = SadTalker(lazy_load=True)
        success_print("Success!!! SadTalkeræ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"SadTalker Error: {e}")
        error_print("è¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")
    
    try:
        from ASR import WhisperASR
        asr = WhisperASR('tiny')
        # from ASR import FunASR
        # asr = FunASR()
        success_print("Success!!! ASRæ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"ASR Error: {e}")
        error_print("è¯·å…ˆä¸‹è½½ASRæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    torch.cuda.empty_cache()

    gr.close_all()
    demo_chatty = app_chatty()
    demo_lively = app_lively()
    demo = gr.TabbedInterface(interface_list = [
                                                demo_chatty,
                                                demo_lively,
                                                ], 
                              tab_names = [
                                            " Chatty_DaJi", 
                                            " Lively_DaJi",
                                           ],
                              title = """
<div style='text-align: left;'>
    <span style='font-size: 28px; '>
        å³¡è°·å°ç‹ä»™â€”â€”â€”å¤šæ¨¡æ€è§’è‰²æ‰®æ¼”å°åŠ©æ‰‹ 
    </span>
</div>
""")
    demo.queue()
    demo.launch(
                #ssl_certfile="/home/xlab-app-center/https_cert/cert.pem",
                #ssl_keyfile="/home/xlab-app-center/https_cert/key.pem",
                #ssl_verify=False,
                share=True
    ) 
